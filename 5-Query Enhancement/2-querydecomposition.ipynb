{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324eafc0",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa45a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4c4917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\ultimate_rag_bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader('langchain_crewai_dataset.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "vectorestore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorestore.as_retriever(search_type = \"mmr\", search_kwargs = {\"k\":4, \"lambda_mult\":0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836daac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d560d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does Langchain use memory and agents compared to CrewAI?\"\n",
    "decompostion_question = decomposition_chain.invoke({\"question\":query}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e895de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the memory architecture utilized by Langchain, and how does it function in its framework?\n",
      "2. How does Langchain implement the use of agents, and what roles do they play in its system?\n",
      "3. What memory features and functionalities does CrewAI offer, and how do they differ from those of Langchain?\n",
      "4. How does the agent-based approach in CrewAI compare to that of Langchain in terms of capabilities and applications?\n"
     ]
    }
   ],
   "source": [
    "print(decompostion_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9493ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the role of memory in Langchain, and how is it implemented in its architecture?']\n",
      "['How does Langchain utilize agents, and what are the key functionalities of these agents?']\n",
      "[\"What is the concept of memory in CrewAI, and how does it differ from Langchain's implementation?\"]\n",
      "['How does CrewAI utilize agents, and what are the main differences in agent functionalities compared to Langchain?']\n"
     ]
    }
   ],
   "source": [
    "for q in decompostion_question.split(\"\\n\"):\n",
    "    if q.strip():\n",
    "        print([q.strip(\"-.1234567890. \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487e0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4:  QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25811f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step5\" Full RAG pipeline logic\n",
    "\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    #Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\":user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "\n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\":subq, \"context\":docs})\n",
    "        results.append(f\"Q: {subq} \\n A:{result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4af56ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: What is the role of memory in LangChain and how is it implemented? \n",
      " A:In LangChain, memory plays a crucial role in enabling the language model (LLM) to maintain context and continuity throughout conversations. It allows the model to keep track of previous conversation turns, ensuring that interactions feel coherent and contextually aware. This is particularly important in applications where the conversation may extend over several exchanges or where users expect personalized responses based on earlier interactions.\n",
      "\n",
      "LangChain implements memory through modules like **ConversationBufferMemory** and **ConversationSummaryMemory**. \n",
      "\n",
      "1. **ConversationBufferMemory**: This module allows the LLM to keep track of the entire conversation history, storing previous exchanges in a buffer. This means that the LLM can reference past interactions directly as the conversation progresses.\n",
      "\n",
      "2. **ConversationSummaryMemory**: In scenarios where conversations are lengthy and may exceed token limits, this module summarizes earlier interactions. By condensing the relevant parts of the conversation into a more manageable form, it ensures that the LLM can still access essential context while adhering to token constraints.\n",
      "\n",
      "Overall, these memory implementations help to enhance user experience by providing relevant responses that take into account the history of the conversation, thus making applications built with LangChain more interactive and user-friendly.\n",
      "\n",
      "Q: How does LangChain utilize agents in its framework? \n",
      " A:LangChain utilizes agents in its framework by employing a planner-executor model, where agents leverage Large Language Models (LLMs) to determine which tools to call, what input to provide, and how to process the resulting output. These agents are capable of executing multi-step tasks and can integrate with various tools, such as web search, calculators, and code execution.\n",
      "\n",
      "The agents can engage in dynamic decision-making, allowing them to plan a sequence of tool invocations to achieve specific goals. They can also implement branching logic and utilize context-aware memory across multiple steps, enhancing their capability to handle complex tasks effectively.\n",
      "\n",
      "Q: What is the role of memory in CrewAI and how does it differ from LangChain's memory? \n",
      " A:The context provided does not explicitly detail the role of memory in CrewAI. However, it indicates that CrewAI integrates with LangChain, which provides memory modules like ConversationBufferMemory and ConversationSummaryMemory that help maintain awareness of previous conversations or summarize long interactions.\n",
      "\n",
      "From this, we can infer that the role of memory in LangChain is focused on tracking conversation history or summarizing interactions to manage context within conversations, primarily for the purpose of enhancing the functionality of LLMs (Large Language Models).\n",
      "\n",
      "On the other hand, the text does not specify how memory functions within CrewAI itself, particularly whether it has its own memory system or relies solely on LangChain's memory. The primary distinction lies in the fact that CrewAI is designed for managing role-based collaboration, potentially using LangChain's memory for context while emphasizing collaborative roles rather than focusing on conversational history or summaries alone.\n",
      "\n",
      "In summary, memory in LangChain is centered around maintaining conversation context, while the role of memory in CrewAI is not clearly defined in the provided context, though it appears to be more about managing collaborative roles in conjunction with LangChain's memory capabilities.\n",
      "\n",
      "Q: How does CrewAI utilize agents and what distinguishes its approach from that of LangChain? \n",
      " A:CrewAI utilizes agents that are defined by a specific purpose, a goal, and a set of tools they can use, ensuring that each agent remains focused and contributes effectively to the overall crew objective. The key distinction in CrewAI's approach compared to LangChain is that while LangChain primarily focuses on retrieval and tool wrapping, CrewAI emphasizes role-based collaboration among agents, facilitating a more coordinated and task-oriented system. This allows CrewAI to manage interactions and contributions among agents in a way that drives towards achieving broader collaborative goals, in contrast to LangChain's functionality.\n"
     ]
    }
   ],
   "source": [
    "#Step6 Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_rag_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
