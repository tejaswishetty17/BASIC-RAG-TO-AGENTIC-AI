{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06f4947",
   "metadata": {},
   "source": [
    "### Query Enhancement â€“ Query Expansion Techniques\n",
    "\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines how good the retrieved context is â€” and therefore, how accurate the LLMâ€™s final answer will be.\n",
    "\n",
    "Thatâ€™s where Query Expansion / Enhancement comes in.\n",
    "\n",
    "#### ðŸŽ¯ What is Query Enhancement?\n",
    "Query enhancement refers to techniques used to improve or reformulate the user query to retrieve better, more relevant documents from the knowledge base.\n",
    "It is especially useful when:\n",
    "\n",
    "- The original query is short, ambiguous, or under-specified\n",
    "- You want to broaden the scope to catch synonyms, related phrases, or spelling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129038fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619a7761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Load and split the dataset\n",
    "\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 50)\n",
    "chunks = splitter.split_documents(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22b5d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\ultimate_rag_bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### Step 2: vectore Store\n",
    "\n",
    "embeding_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeding_model)\n",
    "\n",
    "\n",
    "### step3: MMR Retriever \n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"mmr\",\n",
    "    search_kwargs = {\"k\":5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01de496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020E2BAB2120>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f3904dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000020E70DC6F90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020E70DC72F0>, root_client=<openai.OpenAI object at 0x0000020E70DC68D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020E70DC70B0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: LLM and Prompt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdb21c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000020E70DC6F90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020E70DC72F0>, root_client=<openai.OpenAI object at 0x0000020E70DC68D0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020E70DC70B0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f0f581c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"LangChain memory architecture, persistent memory storage, long-term memory in LangChain, memory management in LangChain, LangChain state management, memory optimization techniques in LangChain, LangChain context retention, algorithms for memory in LangChain, stateful interactions with LangChain, LangChain cognitive memory solutions, LangChain knowledge management, enhanced memory capabilities in LangChain applications, transformer memory mechanisms, LangChain use cases for memory implementation, comparison of memory systems in natural language processing with LangChain.\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77c3b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG answering prompt\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8999b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step5: Full RAG pipeline with query expansion\n",
    "\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x:x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\":x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92c81825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an expanded version of your query to improve document retrieval:\n",
      "\n",
      "```\n",
      "{\n",
      "  'input': 'What types of memory architectures, storage mechanisms, and data retention systems does LangChain support? Specifically, I am looking for information on various memory types such as in-memory storage, persistent storage, short-term memory, long-term memory, and dynamic memory management within LangChain. Additionally, please include technical details about the APIs, connectors, and configurations related to memory implementation in LangChain. Are there any specific use cases, best practices, or examples of memory usage in LangChain applications?'\n",
      "}\n",
      "``` \n",
      "\n",
      "This expanded query includes synonyms and related terms that enhance specificity, targeting various aspects of memory and its implementation within LangChain.\n",
      "âœ… Answer:\n",
      " LangChain supports memory modules such as ConversationBufferMemory and ConversationSummaryMemory. These modules help the LLM maintain awareness of previous conversation turns or summarize long interactions to fit within token limits.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"âœ… Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bdd7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query: \n",
      "\n",
      "\"{'input': 'CrewAI agents, Crew AI personnel, Crew AI virtual assistants, Crew AI autonomous agents, Crew AI robotics, online crew management systems, AI-powered crew coordination, automated crew management solutions, artificial intelligence in crew operations, CrewAI technology, intelligent crew systems, digital crew agents, AI-driven crew optimization, efficiency in crew tasks, virtual crew collaboration tools, tools for enhancing crew performance, advancements in crew AI technology'}\"\n",
      "âœ… Answer:\n",
      " CrewAI agents are autonomous agents that operate within a structured workflow framework. Each agent has a specific roleâ€”such as researcher, planner, or executorâ€”and works semi-independently while collaborating with other agents in the crew. They are defined by a purpose, a goal, and a set of tools to use, ensuring that they stay on task and contribute to the overall objectives of the crew.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"âœ… Answer:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_rag_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
