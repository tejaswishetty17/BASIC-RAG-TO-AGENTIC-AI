{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a16a03",
   "metadata": {},
   "source": [
    "### Hybrid Retriever - Combining Dense And Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddbe9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88bfb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\workspace\\ultimate_rag_bootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# Step 2: Dense Retriever (FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99dc5184",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Sparse Retriever(BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 3 ##top- k documents to retriever\n",
    "\n",
    "### step4 : Combine with Ensemble Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever], \n",
    "    weights=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b71cc63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000197800FD940>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x00000197800FF620>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07387ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Document 1:\n",
      " LangChain helps build LLM applications.\n",
      "\n",
      " Document 2:\n",
      " Langchain can be used to develop agentic ai application.\n",
      "\n",
      " Document 3:\n",
      " Langchain has many types of retrievers.\n",
      "\n",
      " Document 4:\n",
      " Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "### Step 6: Query and get results \n",
    "query= \"How can I build an application using LLM?\"\n",
    "results =hybrid_retriever.invoke(query)\n",
    "\n",
    "# step 6. Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n Document {i+1}:\\n {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f625e4",
   "metadata": {},
   "source": [
    "### RAG pipeline with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027cf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI, init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8ba94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000197837B6490>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000197837B6850>, root_client=<openai.OpenAI object at 0x00000197837B6210>, root_async_client=<openai.AsyncOpenAI object at 0x00000197837B65D0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5 : Prompt Template \n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "                                      \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:{input}                                                                         \n",
    "\"\"\")\n",
    "\n",
    "#step6 - llm\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d59fbb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000197800FD940>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x00000197800FF620>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion:{input}                                                                         \\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000197837B6490>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000197837B6850>, root_client=<openai.OpenAI object at 0x00000197837B6210>, root_async_client=<openai.AsyncOpenAI object at 0x00000197837B65D0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create stuff document chain\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "###create Full rag chain\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain=document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c373f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " To build an app using a Large Language Model (LLM) with LangChain, you can follow these general steps:\n",
      "\n",
      "1. **Define Your Application**: Determine the main purpose of your application and how you want to leverage the LLM. Think about the interactions and features you want to include.\n",
      "\n",
      "2. **Set Up LangChain**: Integrate LangChain into your environment. Install the necessary packages and ensure you have access to an appropriate LLM.\n",
      "\n",
      "3. **Choose a Retriever**: Depending on your application's requirements, select one of the many types of retrievers offered by LangChain. Consider using a vector database like Pinecone for efficient semantic search if your application involves retrieving relevant information.\n",
      "\n",
      "4. **Design the Workflow**: Develop the workflow of your application, focusing on how the LLM will interact with user inputs and how the retriever will fetch relevant data.\n",
      "\n",
      "5. **Implement Agentic AI Features**: If desired, use LangChain to develop agentic AI capabilities that allow your application to perform tasks or make decisions based on user input.\n",
      "\n",
      "6. **Build and Test**: Start coding your application, integrating the LLM and retriever. Continuously test the application to ensure it meets your requirements and performs as expected.\n",
      "\n",
      "7. **Deploy Your Application**: Once fully developed and tested, deploy your application for users to access.\n",
      "\n",
      "By following these steps, you can effectively build an LLM-based application using LangChain.\n",
      "\n",
      " Source Documnets:\n",
      "\n",
      " Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      " Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      " Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      " Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "## Step 9: Ask a question\n",
    "query = {\"input\":\"How can I build an app using LLM?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "#step 10:Outut\n",
    "print(\"Answer:\\n\", response[\"answer\"])\n",
    "print(\"\\n Source Documnets:\")\n",
    "\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\n Doc {i+1}: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultimate_rag_bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
